{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AYGAZ GÖRÜNTÜ İŞLEME BOOTCAMP PROJESİ**\n",
        "\n",
        "Hazırlayan : Duygu Bulut\n"
      ],
      "metadata": {
        "id": "HagJoapFgHnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing and Downloading Dataset:\n",
        "**Kaggle dataset download:**\n",
        "\n",
        "This code imports the kagglehub library and uses it to download the \"Animals with Attributes 2\" dataset from Kaggle. After successful download, a message is printed to confirm that the dataset is ready for use."
      ],
      "metadata": {
        "id": "RJcqJeZTz9c5"
      }
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "rrebirrth_animals_with_attributes_2_path = kagglehub.dataset_download('rrebirrth/animals-with-attributes-2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "MDGKTUZmCtsD",
        "outputId": "62db9b68-fc38-432c-edfd-c9ac5c42ebfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rrebirrth/animals-with-attributes-2?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.0G/13.0G [02:00<00:00, 115MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparing the Dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "Hm75r72t0T0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Organizing Dataset Paths:**\n",
        "\n",
        "This section organizes the dataset by traversing the directory structure to find images of specific animal classes.\n",
        "\n",
        "It creates a dictionary (image_paths) to store file paths for each animal.\n",
        "It iterates through the directory and checks if a folder corresponds to an animal class in the specified list.\n",
        "For matched folders, it collects image file paths and adds them to the dictionary.\n",
        "Finally, it prints the count of images for each class to verify the data."
      ],
      "metadata": {
        "id": "Y9ayA4TqMFSp"
      }
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "5FOPNGRFCtsD",
        "outputId": "6eb8929c-d2e7-4f2b-99b5-6fbdc7f24a9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Initialize the dictionary to hold paths for each animal\n",
        "image_paths = {}\n",
        "\n",
        "# Base directory for your dataset\n",
        "base_path = \"/root/.cache/kagglehub/datasets/rrebirrth/animals-with-attributes-2/versions/1/Animals_with_Attributes2/JPEGImages/\"\n",
        "\n",
        "# List of animals to search for\n",
        "animals = [\"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"]\n",
        "\n",
        "# Traverse the directory structure\n",
        "for dirname, _, filenames in os.walk(base_path):\n",
        "    for animal in animals:\n",
        "        # Check if the current directory contains the animal's name\n",
        "        if animal in dirname:\n",
        "            # Initialize the list if the animal is encountered for the first time\n",
        "            if animal not in image_paths:\n",
        "                image_paths[animal] = []\n",
        "            # Add all image paths for the current animal\n",
        "            for filename in filenames:\n",
        "                image_paths[animal].append(os.path.join(dirname, filename))\n",
        "\n",
        "# Print the paths for verification\n",
        "for animal, paths in image_paths.items():\n",
        "    print(f\"{animal}: {len(paths)} images\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "giant+panda: 874 images\n",
            "polar+bear: 868 images\n",
            "rabbit: 1088 images\n",
            "elephant: 1038 images\n",
            "collie: 1028 images\n",
            "squirrel: 1200 images\n",
            "sheep: 1420 images\n",
            "fox: 664 images\n",
            "moose: 704 images\n",
            "dolphin: 946 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transferring Dataset to Google Drive**\n",
        "\n",
        "This section connects to Google Drive and copies the dataset for easier access and organization.\n",
        "\n",
        "The Drive is mounted to the Colab environment.\n",
        "A target folder is created in Google Drive to store images by class.\n",
        "The code iterates through the dataset directory, copying images from the source directory to the target folder, organizing them by class.\n",
        "A success message is printed to confirm the completion of the transfer."
      ],
      "metadata": {
        "id": "vvqXzjEmMQI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDXuf44VU0FX",
        "outputId": "b6fdbd4f-d614-44b0-c16e-7822ae19e735"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# source dataset path\n",
        "source_base_path = \"/root/.cache/kagglehub/datasets/rrebirrth/animals-with-attributes-2/versions/1/Animals_with_Attributes2/JPEGImages/\"\n",
        "\n",
        "# selected animal classes\n",
        "animals = [\"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"]\n",
        "\n",
        "# create a target folder in drive to use the dataset more easily\n",
        "drive_dataset_path = \"/content/drive/My Drive/bootcamp_project/animal_dataset/\"\n",
        "os.makedirs(drive_dataset_path, exist_ok=True)\n",
        "\n",
        "# copy data containing animal classes\n",
        "for dirname, _, filenames in os.walk(source_base_path):\n",
        "    for animal in animals:\n",
        "        if animal in dirname:\n",
        "           # the target folder will be organized by class name\n",
        "            target_dir = os.path.join(drive_dataset_path, animal)\n",
        "            os.makedirs(target_dir, exist_ok=True)\n",
        "            for filename in filenames:\n",
        "                source_file = os.path.join(dirname, filename)\n",
        "                target_file = os.path.join(target_dir, filename)\n",
        "                # copy the file\n",
        "                shutil.copy2(source_file, target_file)\n",
        "\n",
        "print(f\"Selected classes successfully copied to: {drive_dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODkLHXD1Njjh",
        "outputId": "0feb0af5-6acc-47f0-ad86-7ba9e370c16b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected classes successfully copied to: /content/drive/My Drive/bootcamp_project/animal_dataset/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing Folders and Image Count**\n",
        "\n",
        "This code lists all animal classes and prints the number of images for each class in the dataset. It ensures that only directories (not files) are processed and counts the number of files in each folder."
      ],
      "metadata": {
        "id": "7blTVChINMLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# base folder path where the dataset is located\n",
        "dataset_path = \"/content/drive/My Drive/bootcamp_project/animal_dataset/\"\n",
        "\n",
        "# list folders and image numbers for printing\n",
        "for animal_class in sorted(os.listdir(dataset_path)):\n",
        "    class_path = os.path.join(dataset_path, animal_class)\n",
        "    if os.path.isdir(class_path):  # check the folders\n",
        "        image_count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
        "        print(f\"{animal_class}: {image_count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyU45WLeSs89",
        "outputId": "60aa646f-6b2d-4f65-d071-29e98412ea98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collie: 1028 images\n",
            "dolphin: 946 images\n",
            "elephant: 1038 images\n",
            "fox: 664 images\n",
            "giant+panda: 874 images\n",
            "moose: 704 images\n",
            "polar+bear: 868 images\n",
            "rabbit: 1088 images\n",
            "sheep: 1420 images\n",
            "squirrel: 1200 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1. Create balanced dataset\n",
        "\n",
        "This script balances the dataset by ensuring each animal class has an equal number of images (650 in this case).\n",
        "\n",
        "For each class, it selects the first 650 images.\n",
        "It creates a new directory structure in a specified path to store the balanced dataset.\n",
        "Images are copied from the original dataset to the new balanced dataset directory."
      ],
      "metadata": {
        "id": "vt7TBsvCzkiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# This piece of code is to initially take the first 650 images of each desired animal class and save them to a new folder on the drive\n",
        "\n",
        "# current dataset path\n",
        "dataset_path = \"/content/drive/My Drive/bootcamp_project/animal_dataset/\"\n",
        "\n",
        "# create new balanced dataset path\n",
        "balanced_dataset_path = \"/content/drive/My Drive/bootcamp_project/balanced_animal_dataset/\"\n",
        "os.makedirs(balanced_dataset_path, exist_ok=True)\n",
        "\n",
        "# target number per image\n",
        "target_count = 650\n",
        "\n",
        "# moving selected files from each class to a new folder\n",
        "for animal_class in sorted(os.listdir(dataset_path)):\n",
        "    class_path = os.path.join(dataset_path, animal_class)\n",
        "    if os.path.isdir(class_path):  # process folders only\n",
        "        # get all image files in folder in order\n",
        "        all_images = sorted([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
        "        # select first 650 images\n",
        "        selected_images = all_images[:target_count]\n",
        "        # create a folder for the class in the new destination folder\n",
        "        target_class_path = os.path.join(balanced_dataset_path, animal_class)\n",
        "        os.makedirs(target_class_path, exist_ok=True)\n",
        "        # copy selected files to new folder\n",
        "        for image in selected_images:\n",
        "            source_file = os.path.join(class_path, image)\n",
        "            target_file = os.path.join(target_class_path, image)\n",
        "            shutil.copy2(source_file, target_file)\n",
        "\n",
        "        print(f\"{animal_class}: {len(selected_images)} images copied\")\n",
        "\n",
        "print(\"Balanced dataset creation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzv1HkX1TzOk",
        "outputId": "603310e3-3f3f-469c-dfa6-7e293a3e6aaa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collie: 650 images copied\n",
            "dolphin: 650 images copied\n",
            "elephant: 650 images copied\n",
            "fox: 650 images copied\n",
            "giant+panda: 650 images copied\n",
            "moose: 650 images copied\n",
            "polar+bear: 650 images copied\n",
            "rabbit: 650 images copied\n",
            "sheep: 650 images copied\n",
            "squirrel: 650 images copied\n",
            "Balanced dataset creation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2. Resizing and Normalizing Dataset\n",
        "\n",
        "This function resizes and normalizes images in the dataset.\n",
        "\n",
        "It resizes images to a fixed size (e.g., 64x64 pixels).\n",
        "It normalizes pixel values to a range of [0, 1].\n",
        "Resized images are saved in a new directory, maintaining the original directory structure."
      ],
      "metadata": {
        "id": "A7LDyt1xzZl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To normalize all images to the same size.\n",
        "Resize images to the appropriate size according to the input layer of the model.\n",
        "'''\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# image resizing and saving function\n",
        "def resize_and_save_images(input_path, output_path, image_size=(128, 128)):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    # loop through all image files\n",
        "    for root, dirs, files in os.walk(input_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                input_image = cv2.imread(file_path)\n",
        "                # resize if input image is not none\n",
        "                if input_image is not None:\n",
        "                    img_resized = cv2.resize(input_image, image_size)\n",
        "                    img_normalized = img_resized / 255.0  # normalization\n",
        "                    relative_path = os.path.relpath(root, input_path)\n",
        "                    save_dir = os.path.join(output_path, relative_path)\n",
        "                    # create save_dir if it doesn't exist\n",
        "                    if not os.path.exists(save_dir):\n",
        "                        os.makedirs(save_dir)\n",
        "                    # save images to output_file_path\n",
        "                    output_file_path = os.path.join(save_dir, file)\n",
        "                    cv2.imwrite(output_file_path, (img_normalized * 255).astype(np.uint8))\n",
        "\n",
        "# input and output dataset paths\n",
        "input_dataset_path = \"/content/drive/My Drive/bootcamp_project/balanced_animal_dataset/\"\n",
        "output_dataset_path = \"/content/processed_data\"\n",
        "\n",
        "# call the function\n",
        "resize_and_save_images(input_dataset_path, output_dataset_path, image_size=(64, 64))"
      ],
      "metadata": {
        "id": "KQFwbUQVkemc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3. Preparation and splitting of data for training and test dataset\n",
        "\n",
        "This code prepares the dataset for training:\n",
        "\n",
        "Loads images and their labels, associating each class name with a unique numerical label.\n",
        "Normalizes images.\n",
        "Splits the dataset into training (70%) and test (30%) subsets using train_test_split."
      ],
      "metadata": {
        "id": "8KkZnatvzBnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# preparation of data and labels\n",
        "def load_processed_dataset(data_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(data_path))  # classes in alphabetical order\n",
        "    class_to_label = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "    for cls_name in class_names:\n",
        "        class_path = os.path.join(data_path, cls_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            for file in os.listdir(class_path):\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    file_path = os.path.join(class_path, file)\n",
        "                    img = cv2.imread(file_path)  # Image resized and normalized\n",
        "                    if img is not None:\n",
        "                        images.append(img / 255.0)  # make sure it is normalized for validation\n",
        "                        labels.append(class_to_label[cls_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# load the dataset\n",
        "processed_dataset_path = \"/content/processed_data\"  # Processed data path\n",
        "X, y, class_names = load_processed_dataset(processed_dataset_path)\n",
        "\n",
        "# splitting into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# printing results\n",
        "print(f\"Toplam veri sayısı: {len(X)}\")\n",
        "print(f\"Eğitim veri sayısı: {len(X_train)}\")\n",
        "print(f\"Test veri sayısı: {len(X_test)}\")\n",
        "print(f\"Sınıflar: {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wxrkLknt0i3",
        "outputId": "f07decb0-e9bb-4262-fc6b-92ce1389a44b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam veri sayısı: 6500\n",
            "Eğitim veri sayısı: 4550\n",
            "Test veri sayısı: 1950\n",
            "Sınıflar: ['collie', 'dolphin', 'elephant', 'fox', 'giant+panda', 'moose', 'polar+bear', 'rabbit', 'sheep', 'squirrel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4. Data manipulation and augmentation\n",
        "\n",
        "This code performs data augmentation to artificially expand the training dataset:\n",
        "\n",
        "Applies random transformations (rotation, zoom, flip, and translation) to the training images.\n",
        "Creates a specified number of augmented versions of each image.\n",
        "Combines the original and augmented datasets, increasing the variety in the training data to improve model robustness."
      ],
      "metadata": {
        "id": "voYlrBN5xtf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Data augmentation settings for tensorflow layers\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomRotation(0.1),  # ±10% rotate\n",
        "    tf.keras.layers.RandomZoom(0.1),      # ±10% zoom\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),  # horizontal flip\n",
        "    tf.keras.layers.RandomTranslation(0.1, 0.1),  # width/height shift\n",
        "])\n",
        "\n",
        "# applying data augmentation to the training set\n",
        "def augment_images_tf(X_train, y_train, num_augmented=5):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for img, label in zip(X_train, y_train):\n",
        "        img = tf.expand_dims(img, axis=0)  # # to make the format suitable for TensorFlow\n",
        "        for _ in range(num_augmented):\n",
        "            augmented_img = data_augmentation(img, training=True)\n",
        "            augmented_images.append(augmented_img[0].numpy())\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_images), np.array(augmented_labels)\n",
        "\n",
        "# data augmentation process\n",
        "# num_augmented : Specified number of increments for each image\n",
        "augmented_X_train, augmented_y_train = augment_images_tf(X_train, y_train, num_augmented=5)\n",
        "\n",
        "# combining the results\n",
        "X_train_augmented = np.concatenate((X_train, augmented_X_train), axis=0)\n",
        "y_train_augmented = np.concatenate((y_train, augmented_y_train), axis=0)\n",
        "# print shape of the new dataset\n",
        "print(f\"Orijinal eğitim veri boyutu: {X_train.shape}\")\n",
        "print(f\"Artırılmış eğitim veri boyutu: {X_train_augmented.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa3gWC8FxH2a",
        "outputId": "c1ff2bbc-1a67-45f6-a4a2-69c9eacfb68d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orijinal eğitim veri boyutu: (4550, 64, 64, 3)\n",
            "Artırılmış eğitim veri boyutu: (27300, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Designing the CNN Model:"
      ],
      "metadata": {
        "id": "BvNAjBLw6PHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Building a Convolutional Neural Network (CNN) with TensorFlow\n",
        "\n",
        "This CNN is designed for multi-class classification of images (10 classes). It processes the input through convolutional, pooling, and fully connected layers to extract features, reduce dimensionality, and classify the images based on their features.\n",
        "\n",
        "This code constructs a CNN model for classifying images into 10 classes. Here's a breakdown of its components:\n",
        "1. Input Layer and First Convolutional Block\n",
        "\n",
        "Conv2D(32, (3, 3), activation='relu'):\n",
        "Adds a convolutional layer with 32 filters, each of size 3x3.\n",
        "Applies the ReLU activation function to introduce non-linearity.\n",
        "input_shape=(64, 64, 3):\n",
        "Specifies the input image dimensions (64x64 pixels with 3 color channels for RGB).\n",
        "MaxPooling2D((2, 2)):\n",
        "Reduces the spatial dimensions by half, making the model computationally efficient and focusing on significant features.\n",
        "\n",
        "2. Second Convolutional Block\n",
        "\n",
        "Conv2D(64, (3, 3), activation='relu'):\n",
        "Adds another convolutional layer with 64 filters.\n",
        "MaxPooling2D((2, 2)):\n",
        "Further reduces the spatial dimensions.\n",
        "\n",
        "3. Third Convolutional Layer\n",
        "\n",
        "Conv2D(128, (3, 3), activation='relu'):\n",
        "Introduces a deeper layer with 128 filters to capture more complex features.\n",
        "\n",
        "4. Flattening and Fully Connected Layers\n",
        "\n",
        "Flatten():\n",
        "Converts the 2D feature maps into a 1D vector to pass into fully connected layers.\n",
        "Dense(128, activation='relu'):\n",
        "Adds a dense layer with 128 neurons and ReLU activation for further learning.\n",
        "Dropout(0.5):\n",
        "Randomly drops 50% of neurons during training to prevent overfitting.\n",
        "\n",
        "5. Output Layer\n",
        "\n",
        "Dense(10, activation='softmax'):\n",
        "Adds the final dense layer with 10 neurons (one for each class).\n",
        "Uses the softmax activation function to output probabilities for each class.\n",
        "\n",
        "6. Model Summary\n",
        "\n",
        "Displays a summary of the model architecture, including the number of parameters for each layer and total parameters."
      ],
      "metadata": {
        "id": "jV8tRRwz7Oas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Creating the model\n",
        "model = models.Sequential([\n",
        "    # First convolutional layer and pooling layer\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),  # 64x64 images and 3 color channels (RGB)\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Second convolutional layer and pooling layer\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Third convolutional layer\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    # Flatten for switching to fully connected layers\n",
        "    layers.Flatten(),\n",
        "    # Fully connected layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    # Dropout layer\n",
        "    layers.Dropout(0.5),\n",
        "    # Output layer (softmax activation for 10 classes)\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# View model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "byPRDKjo6E2L",
        "outputId": "b8377777-0f6e-4f87-f02c-073595b9b5bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18432\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m2,359,424\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18432</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,424</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,453,962\u001b[0m (9.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,453,962</span> (9.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,453,962\u001b[0m (9.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,453,962</span> (9.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 2. Compiling the Model\n",
        "\n",
        "optimizer='adam':\n",
        "Uses the Adam optimizer, a widely used algorithm for fast and adaptive learning during backpropagation.\n",
        "loss='categorical_crossentropy':\n",
        "Suitable loss function for multi-class classification tasks where the target labels are one-hot encoded.\n",
        "metrics=['accuracy']:\n",
        "Tracks the model's accuracy as a performance metric during training and evaluation."
      ],
      "metadata": {
        "id": "oqKG7i7j7mz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',  # Adam optimizer for fast learning\n",
        "    loss='categorical_crossentropy',  # Suitable for multi-class classification\n",
        "    metrics=['accuracy']  # Accuracy for performance evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "w7aPVSSK7cVf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Training the Model\n",
        "\n",
        "X_train_augmented:\n",
        "Augmented training images are used to enhance model robustness and reduce overfitting.\n",
        "\n",
        "tf.keras.utils.to_categorical(y_train_augmented, num_classes=10):\n",
        "Converts integer labels to one-hot encoded format for compatibility with the loss function.\n",
        "For example, label 3 becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
        "\n",
        "validation_data:\n",
        "Provides validation data (test images and one-hot encoded test labels) to monitor the model's performance on unseen data after each epoch.\n",
        "\n",
        "epochs=20:\n",
        "Trains the model for 20 iterations over the entire training dataset.\n",
        "\n",
        "batch_size=32:\n",
        "Divides the dataset into smaller batches of 32 samples for gradient updates, improving training efficiency.\n"
      ],
      "metadata": {
        "id": "xaqbjYzz7vcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model\n",
        "history = model.fit(\n",
        "    X_train_augmented,  # Training images (augmented)\n",
        "    tf.keras.utils.to_categorical(y_train_augmented, num_classes=10),  # Tags one-hot encoding\n",
        "    validation_data=(\n",
        "        X_test,\n",
        "        tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "    ),  # Test data\n",
        "    epochs=100,  # Epoch\n",
        "    batch_size=16  # Batch size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-QxAa7V7ywx",
        "outputId": "46981585-6852-404d-c445-ccdc51257ac4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - accuracy: 0.9508 - loss: 0.1601 - val_accuracy: 0.6856 - val_loss: 3.6281\n",
            "Epoch 2/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.9464 - loss: 0.1725 - val_accuracy: 0.6718 - val_loss: 3.3247\n",
            "Epoch 3/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9494 - loss: 0.1512 - val_accuracy: 0.6882 - val_loss: 3.3218\n",
            "Epoch 4/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9458 - loss: 0.1676 - val_accuracy: 0.6795 - val_loss: 3.1953\n",
            "Epoch 5/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9477 - loss: 0.1749 - val_accuracy: 0.6677 - val_loss: 3.1404\n",
            "Epoch 6/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9527 - loss: 0.1542 - val_accuracy: 0.6831 - val_loss: 2.9627\n",
            "Epoch 7/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9461 - loss: 0.1723 - val_accuracy: 0.6600 - val_loss: 2.8993\n",
            "Epoch 8/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9475 - loss: 0.1578 - val_accuracy: 0.6810 - val_loss: 2.8529\n",
            "Epoch 9/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9521 - loss: 0.1516 - val_accuracy: 0.6723 - val_loss: 3.5748\n",
            "Epoch 10/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9541 - loss: 0.1509 - val_accuracy: 0.6836 - val_loss: 3.4193\n",
            "Epoch 11/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1519 - val_accuracy: 0.6867 - val_loss: 3.3496\n",
            "Epoch 12/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9484 - loss: 0.1690 - val_accuracy: 0.6831 - val_loss: 3.4613\n",
            "Epoch 13/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9575 - loss: 0.1396 - val_accuracy: 0.6846 - val_loss: 3.7483\n",
            "Epoch 14/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9550 - loss: 0.1494 - val_accuracy: 0.6785 - val_loss: 3.2787\n",
            "Epoch 15/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9563 - loss: 0.1477 - val_accuracy: 0.6815 - val_loss: 2.9723\n",
            "Epoch 16/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9564 - loss: 0.1344 - val_accuracy: 0.6682 - val_loss: 3.1061\n",
            "Epoch 17/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9569 - loss: 0.1390 - val_accuracy: 0.6867 - val_loss: 3.1614\n",
            "Epoch 18/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9526 - loss: 0.1543 - val_accuracy: 0.6574 - val_loss: 3.6720\n",
            "Epoch 19/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9503 - loss: 0.1612 - val_accuracy: 0.6718 - val_loss: 3.1530\n",
            "Epoch 20/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9587 - loss: 0.1268 - val_accuracy: 0.6872 - val_loss: 4.1174\n",
            "Epoch 21/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9581 - loss: 0.1266 - val_accuracy: 0.6872 - val_loss: 3.0151\n",
            "Epoch 22/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9590 - loss: 0.1380 - val_accuracy: 0.6908 - val_loss: 3.8420\n",
            "Epoch 23/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9565 - loss: 0.1449 - val_accuracy: 0.6800 - val_loss: 3.6783\n",
            "Epoch 24/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9582 - loss: 0.1299 - val_accuracy: 0.6759 - val_loss: 3.4801\n",
            "Epoch 25/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9582 - loss: 0.1268 - val_accuracy: 0.6723 - val_loss: 3.1204\n",
            "Epoch 26/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9586 - loss: 0.1427 - val_accuracy: 0.6728 - val_loss: 3.2630\n",
            "Epoch 27/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9566 - loss: 0.1419 - val_accuracy: 0.6718 - val_loss: 4.1005\n",
            "Epoch 28/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.1263 - val_accuracy: 0.6903 - val_loss: 3.7423\n",
            "Epoch 29/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9611 - loss: 0.1260 - val_accuracy: 0.6841 - val_loss: 3.9283\n",
            "Epoch 30/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1287 - val_accuracy: 0.6877 - val_loss: 3.5427\n",
            "Epoch 31/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1232 - val_accuracy: 0.6985 - val_loss: 3.9111\n",
            "Epoch 32/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.1330 - val_accuracy: 0.6713 - val_loss: 3.3760\n",
            "Epoch 33/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9612 - loss: 0.1271 - val_accuracy: 0.6872 - val_loss: 3.5091\n",
            "Epoch 34/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1280 - val_accuracy: 0.6682 - val_loss: 4.5145\n",
            "Epoch 35/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1286 - val_accuracy: 0.6928 - val_loss: 3.5668\n",
            "Epoch 36/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9622 - loss: 0.1267 - val_accuracy: 0.6892 - val_loss: 3.4812\n",
            "Epoch 37/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9594 - loss: 0.1430 - val_accuracy: 0.6856 - val_loss: 4.1381\n",
            "Epoch 38/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9648 - loss: 0.1153 - val_accuracy: 0.6744 - val_loss: 3.7715\n",
            "Epoch 39/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1196 - val_accuracy: 0.6892 - val_loss: 4.2195\n",
            "Epoch 40/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1269 - val_accuracy: 0.6610 - val_loss: 3.4400\n",
            "Epoch 41/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.1280 - val_accuracy: 0.6815 - val_loss: 4.6120\n",
            "Epoch 42/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.1253 - val_accuracy: 0.6744 - val_loss: 3.4731\n",
            "Epoch 43/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.1127 - val_accuracy: 0.6738 - val_loss: 4.0598\n",
            "Epoch 44/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9625 - loss: 0.1271 - val_accuracy: 0.6821 - val_loss: 4.2118\n",
            "Epoch 45/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.1282 - val_accuracy: 0.6831 - val_loss: 3.6283\n",
            "Epoch 46/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.1251 - val_accuracy: 0.6918 - val_loss: 4.2245\n",
            "Epoch 47/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.1148 - val_accuracy: 0.6692 - val_loss: 3.8583\n",
            "Epoch 48/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9626 - loss: 0.1240 - val_accuracy: 0.6856 - val_loss: 4.0272\n",
            "Epoch 49/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9632 - loss: 0.1324 - val_accuracy: 0.6754 - val_loss: 4.2725\n",
            "Epoch 50/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1264 - val_accuracy: 0.6831 - val_loss: 4.0023\n",
            "Epoch 51/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9623 - loss: 0.1232 - val_accuracy: 0.6882 - val_loss: 4.1223\n",
            "Epoch 52/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.1236 - val_accuracy: 0.6923 - val_loss: 4.2300\n",
            "Epoch 53/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.1181 - val_accuracy: 0.6779 - val_loss: 4.1003\n",
            "Epoch 54/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9666 - loss: 0.1154 - val_accuracy: 0.6785 - val_loss: 4.0851\n",
            "Epoch 55/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.1156 - val_accuracy: 0.6718 - val_loss: 4.2047\n",
            "Epoch 56/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.1201 - val_accuracy: 0.6928 - val_loss: 5.3832\n",
            "Epoch 57/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1255 - val_accuracy: 0.6759 - val_loss: 4.2472\n",
            "Epoch 58/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1479 - val_accuracy: 0.6856 - val_loss: 4.6533\n",
            "Epoch 59/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1364 - val_accuracy: 0.6738 - val_loss: 4.2224\n",
            "Epoch 60/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9636 - loss: 0.1163 - val_accuracy: 0.6764 - val_loss: 3.7965\n",
            "Epoch 61/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9651 - loss: 0.1159 - val_accuracy: 0.6733 - val_loss: 5.1617\n",
            "Epoch 62/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9644 - loss: 0.1351 - val_accuracy: 0.6656 - val_loss: 4.7639\n",
            "Epoch 63/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9707 - loss: 0.1062 - val_accuracy: 0.6779 - val_loss: 3.9294\n",
            "Epoch 64/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1254 - val_accuracy: 0.6641 - val_loss: 4.2956\n",
            "Epoch 65/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.1213 - val_accuracy: 0.6785 - val_loss: 3.9207\n",
            "Epoch 66/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1117 - val_accuracy: 0.6667 - val_loss: 4.9513\n",
            "Epoch 67/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.1226 - val_accuracy: 0.6764 - val_loss: 4.0091\n",
            "Epoch 68/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.1119 - val_accuracy: 0.6697 - val_loss: 4.5477\n",
            "Epoch 69/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.1101 - val_accuracy: 0.6687 - val_loss: 4.4894\n",
            "Epoch 70/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9618 - loss: 0.1407 - val_accuracy: 0.6723 - val_loss: 5.4832\n",
            "Epoch 71/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9683 - loss: 0.1242 - val_accuracy: 0.6785 - val_loss: 4.6034\n",
            "Epoch 72/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.1240 - val_accuracy: 0.6733 - val_loss: 5.0455\n",
            "Epoch 73/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9598 - loss: 0.1513 - val_accuracy: 0.6667 - val_loss: 5.3493\n",
            "Epoch 74/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9648 - loss: 0.1255 - val_accuracy: 0.6656 - val_loss: 4.1452\n",
            "Epoch 75/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.1216 - val_accuracy: 0.6662 - val_loss: 5.5504\n",
            "Epoch 76/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.0982 - val_accuracy: 0.6672 - val_loss: 4.8760\n",
            "Epoch 77/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.1012 - val_accuracy: 0.6851 - val_loss: 5.3862\n",
            "Epoch 78/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.1005 - val_accuracy: 0.6738 - val_loss: 4.9421\n",
            "Epoch 79/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9648 - loss: 0.1358 - val_accuracy: 0.6826 - val_loss: 4.4545\n",
            "Epoch 80/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.1095 - val_accuracy: 0.6723 - val_loss: 4.8401\n",
            "Epoch 81/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.1212 - val_accuracy: 0.6815 - val_loss: 5.0249\n",
            "Epoch 82/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.1110 - val_accuracy: 0.6631 - val_loss: 4.5726\n",
            "Epoch 83/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1281 - val_accuracy: 0.6718 - val_loss: 4.7026\n",
            "Epoch 84/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9687 - loss: 0.1111 - val_accuracy: 0.6897 - val_loss: 4.9196\n",
            "Epoch 85/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9689 - loss: 0.1199 - val_accuracy: 0.6723 - val_loss: 5.1175\n",
            "Epoch 86/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9694 - loss: 0.1127 - val_accuracy: 0.6754 - val_loss: 5.2031\n",
            "Epoch 87/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.1353 - val_accuracy: 0.6595 - val_loss: 4.6770\n",
            "Epoch 88/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.1066 - val_accuracy: 0.6713 - val_loss: 5.1332\n",
            "Epoch 89/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.1040 - val_accuracy: 0.6708 - val_loss: 4.3318\n",
            "Epoch 90/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1437 - val_accuracy: 0.6810 - val_loss: 4.9776\n",
            "Epoch 91/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0993 - val_accuracy: 0.6585 - val_loss: 5.9396\n",
            "Epoch 92/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9705 - loss: 0.1098 - val_accuracy: 0.6774 - val_loss: 5.0470\n",
            "Epoch 93/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9687 - loss: 0.1191 - val_accuracy: 0.6738 - val_loss: 5.1874\n",
            "Epoch 94/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.1106 - val_accuracy: 0.6697 - val_loss: 4.9385\n",
            "Epoch 95/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.1143 - val_accuracy: 0.6662 - val_loss: 5.7543\n",
            "Epoch 96/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9627 - loss: 0.1366 - val_accuracy: 0.6723 - val_loss: 5.0864\n",
            "Epoch 97/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0927 - val_accuracy: 0.6728 - val_loss: 5.5619\n",
            "Epoch 98/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9683 - loss: 0.1170 - val_accuracy: 0.6749 - val_loss: 4.9604\n",
            "Epoch 99/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.1302 - val_accuracy: 0.6764 - val_loss: 5.3165\n",
            "Epoch 100/100\n",
            "\u001b[1m1707/1707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9677 - loss: 0.1177 - val_accuracy: 0.6800 - val_loss: 5.2948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for saving to drive and loading model"
      ],
      "metadata": {
        "id": "7I9yM2eimQL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model save path\n",
        "model_save_path = \"/content/drive/My Drive/bootcamp_project/saved_model.h5\"\n",
        "\n",
        "# save the model\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "# load the model\n",
        "model = load_model(\"/content/drive/My Drive/bootcamp_project/saved_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OUrPlt3mVJu",
        "outputId": "afa7d3be-82e3-4088-d0ee-3375e19b72b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at /content/drive/My Drive/bootcamp_project/saved_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Testing the Model:\n",
        "\n",
        "Tests the model on unseen data and reports accuracy and loss to measure generalization performance.\n",
        "\n",
        "**model.evaluate**:\n",
        "Evaluates the model's performance on the test dataset.\n",
        "Returns the test loss and test accuracy.\n",
        "\n",
        "**X_test**:\n",
        "Test images are used for evaluation.\n",
        "\n",
        "**tf.keras.utils.to_categorical(y_test, num_classes=10)**:\n",
        "Converts test labels to one-hot encoded format.\n",
        "\n",
        "**test_accuracy**:\n",
        "Displays the model's accuracy on the test dataset as a percentage (e.g., 92.35%).\n",
        "\n",
        "**test_loss**:\n",
        "Outputs the loss value, representing the model's error on the test dataset.\n"
      ],
      "metadata": {
        "id": "ncJTuF18-eMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    X_test,\n",
        "    tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        ")\n",
        "print(f\"Test Doğruluğu: {test_accuracy:.2%}\")\n",
        "print(f\"Test Kaybı: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taldTFfb-jTD",
        "outputId": "1d27cdf4-4a2e-4c31-f748-035d9f656808"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6756 - loss: 5.5746\n",
            "Test Doğruluğu: 68.00%\n",
            "Test Kaybı: 5.2948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Manipulating Images with Different Lights:"
      ],
      "metadata": {
        "id": "wM6a9kYXBJfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Color Constancy\n",
        "\"\"\"\n",
        "Global AI Image Processing Bootcamp\n",
        "Dec 2024\n",
        "Diclehan and Oguzhan Ulucan\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "### Fundamental functions\n",
        "def linearize_image(image):\n",
        "    \"\"\"\n",
        "    Converts an sRGB image to linear RGB assuming the input image is in the range [0, 1]\n",
        "    \"\"\"\n",
        "    return np.where(image <= 0.04045,\n",
        "                    image / 12.92,\n",
        "                    ((image + 0.055) / 1.055) ** 2.4)\n",
        "\n",
        "def linear_to_srgb(image):\n",
        "    \"\"\"\n",
        "    Converts a linear RGB image to sRGB assuming the input image is in the range [0, 1]\n",
        "    \"\"\"\n",
        "    return np.where(image <= 0.0031308,\n",
        "                    image * 12.92,\n",
        "                    1.055 * (image ** (1 / 2.4)) - 0.055)\n",
        "\n",
        "def handle_saturation(image, lower=0.05, upper=0.95):\n",
        "    \"\"\"\n",
        "    Creates a mask for non-saturated pixels (those between `lower` and `upper` thresholds)\n",
        "    \"\"\"\n",
        "    return np.all((image > lower) & (image < upper), axis=-1)\n",
        "\n",
        "### Color constancy\n",
        "def estimate_light_source_grey_world(image, mask):\n",
        "    \"\"\"\n",
        "    Estimates the light source based on the Grey World assumption, using valid pixels from the mask\n",
        "    \"\"\"\n",
        "    valid_pixels = image[mask]\n",
        "    avg_color = np.mean(valid_pixels, axis=0)\n",
        "    return avg_color / np.linalg.norm(avg_color)\n",
        "\n",
        "def correct_colors(image, light_source):\n",
        "    \"\"\"\n",
        "    Corrects the colors of the image by applying white balance using the estimated light source\n",
        "    \"\"\"\n",
        "    return image * (1.0 / light_source)\n",
        "\n",
        "def manipulate_light_source(image, light_color):\n",
        "    \"\"\"\n",
        "    Simulates color manipulation under a different light source\n",
        "\n",
        "    Args:\n",
        "    - image: The input image (sRGB, [0, 1])\n",
        "    - light_color: The light source color (unit norm RGB vector)\n",
        "\n",
        "    Returns:\n",
        "    - Manipulated image (sRGB, [0, 1])\n",
        "    \"\"\"\n",
        "    # Step 1: Linearize the image\n",
        "    linear_image = linearize_image(image)\n",
        "    # Step 2: Apply the light source (multiplying the linear image by the light color)\n",
        "    manipulated_image = linear_image * light_color\n",
        "    # Step 3: Convert the manipulated image back to sRGB\n",
        "    manipulated_srgb = linear_to_srgb(manipulated_image)\n",
        "\n",
        "    return np.clip(manipulated_srgb, 0, 1)\n",
        "\n",
        "def process_and_white_balance(image):\n",
        "    \"\"\"\n",
        "    Applies white balance using both the Grey World and Max RGB methods\n",
        "    Returns both corrected images in sRGB format\n",
        "    \"\"\"\n",
        "    linear_image = linearize_image(image)\n",
        "    valid_mask = handle_saturation(linear_image)\n",
        "\n",
        "    # Light source estimations\n",
        "    grey_world_light = estimate_light_source_grey_world(linear_image, valid_mask)\n",
        "    # Color correction using both light sources\n",
        "    corrected_grey_world = correct_colors(linear_image, grey_world_light)\n",
        "    # Convert back to sRGB\n",
        "    srgb_grey_world = linear_to_srgb(corrected_grey_world)\n",
        "    # Clip and return\n",
        "    return np.clip(srgb_grey_world, 0, 1)\n",
        "\n",
        "# Light sources for color manipulation\n",
        "def get_light_sources():\n",
        "    \"\"\"\n",
        "    Returns a set of light sources for image manipulation\n",
        "    \"\"\"\n",
        "    purplish_light = np.array([0.82, 0.15, 0.89]) / np.linalg.norm([0.82, 0.15, 0.89])\n",
        "    yellowish_light = np.array([0.96, 0.24, 0.11]) / np.linalg.norm([0.96, 0.24, 0.11])\n",
        "    greenish_light = np.array([0.11, 0.98, 0.12]) / np.linalg.norm([0.11, 0.98, 0.12])\n",
        "    return purplish_light, yellowish_light, greenish_light\n",
        "\n",
        "\n",
        "def get_wb_images(image):\n",
        "    \"\"\"\n",
        "    Process and white balance the image\n",
        "    \"\"\"\n",
        "    srgb_grey_world = process_and_white_balance(image)\n",
        "    # Save the white-balanced images\n",
        "    #cv2.imwrite('white_balanced_grey_world.jpg', cv2.cvtColor((srgb_grey_world*255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "    return srgb_grey_world\n",
        "\n",
        "def get_manipulated_images(image):\n",
        "    \"\"\"\n",
        "    Get manipulated images by applying color vectors\n",
        "    \"\"\"\n",
        "    # Get the color vectors\n",
        "    purplish_light, orangish_light, greenish_light = get_light_sources()\n",
        "\n",
        "    # Manipulate the images under different light sources\n",
        "    manipulated_purplish = manipulate_light_source(image, purplish_light)\n",
        "    manipulated_orangish = manipulate_light_source(image, orangish_light)\n",
        "    manipulated_greenish = manipulate_light_source(image, greenish_light)\n",
        "\n",
        "    # Save the manipulated images\n",
        "    #cv2.imwrite('manipulated_purplish.jpg', cv2.cvtColor((manipulated_purplish*255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "    #cv2.imwrite('manipulated_orangish.jpg', cv2.cvtColor((manipulated_orangish*255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "    #cv2.imwrite('manipulated_greenish.jpg', cv2.cvtColor((manipulated_greenish*255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "    return manipulated_purplish, manipulated_orangish, manipulated_greenish\n"
      ],
      "metadata": {
        "id": "E5VveADTBMa6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new test set for manipulation\n",
        "X_test_manipulated = []\n",
        "y_test_manipulated = []\n",
        "\n",
        "for i, image in enumerate(X_test):\n",
        "    # Give the normalized image to the manipulation function\n",
        "    manipulated_purplish, manipulated_orangish, manipulated_greenish = get_manipulated_images(image)\n",
        "    # Add manipulated images to list\n",
        "    X_test_manipulated.append(manipulated_purplish)\n",
        "    X_test_manipulated.append(manipulated_orangish)\n",
        "    X_test_manipulated.append(manipulated_greenish)\n",
        "    # Duplicate labels\n",
        "    y_test_manipulated.extend([y_test[i]] * 3)\n",
        "\n",
        "# convert from list to numpy array\n",
        "X_test_manipulated = np.array(X_test_manipulated)\n",
        "y_test_manipulated = np.array(y_test_manipulated)\n",
        "\n",
        "print(f\"Manipüle edilmiş X_test şekli: {X_test_manipulated.shape}\")\n",
        "print(f\"Manipüle edilmiş y_test şekli: {y_test_manipulated.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNcbhIQdrAQ9",
        "outputId": "64968aa8-b667-4ae3-eb04-f51dad89b4c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manipüle edilmiş X_test şekli: (5850, 64, 64, 3)\n",
            "Manipüle edilmiş y_test şekli: (5850,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Testing the Model with Manipulated Test Set:"
      ],
      "metadata": {
        "id": "NE0CrbgVBNlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print manipulated dataset shape\n",
        "print(f\"X_test_manipulated şekli: {X_test_manipulated.shape}\")\n",
        "print(f\"y_test_manipulated şekli: {y_test_manipulated.shape}\")\n",
        "\n",
        "# Model değerlendirme\n",
        "y_test_manipulated_one_hot = tf.keras.utils.to_categorical(y_test_manipulated, num_classes=10)\n",
        "\n",
        "# Modeli değerlendir\n",
        "test_loss, test_accuracy = model.evaluate(X_test_manipulated, y_test_manipulated_one_hot)\n",
        "print(f\"Manipüle edilmiş test setindeki kayıp: {test_loss}\")\n",
        "print(f\"Manipüle edilmiş test setindeki doğruluk: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4DATF3KBXFy",
        "outputId": "602a9721-9e9a-4398-a5ec-4b56aa09bb04"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test_manipulated şekli: (5850, 64, 64, 3)\n",
            "y_test_manipulated şekli: (5850,)\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1862 - loss: 54.2937\n",
            "Manipüle edilmiş test setindeki kayıp: 54.973899841308594\n",
            "Manipüle edilmiş test setindeki doğruluk: 0.18905982375144958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Applying Color Constancy Algorithm to Manipulated Test Set:"
      ],
      "metadata": {
        "id": "9aCV0_h1Bj_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_white_balance_to_test_set(X_test_manipulated):\n",
        "    \"\"\"\n",
        "    Apply white balance to all images in the manipulated test set.\n",
        "    \"\"\"\n",
        "    wb_images = []\n",
        "    for image in X_test_manipulated:\n",
        "        # Apply white balance using get_wb_images\n",
        "        wb_image = get_wb_images(image)\n",
        "        wb_images.append(wb_image)\n",
        "    return np.array(wb_images)\n",
        "\n",
        "# apply\n",
        "X_test_wb = apply_white_balance_to_test_set(X_test_manipulated)\n",
        "\n",
        "# check the results\n",
        "print(f\"Renk sabitliği uygulanmış X_test_wb şekli: {X_test_wb.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gu7NaWmBpGD",
        "outputId": "9f69924e-0a9c-41b6-cce1-1d2f7ef42a5d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renk sabitliği uygulanmış X_test_wb şekli: (5850, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Testing the Model with the Color Consistency Applied Test Set:"
      ],
      "metadata": {
        "id": "pg5yyZSqBvmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the color-consistent test set\n",
        "test_loss_wb, test_accuracy_wb = model.evaluate(\n",
        "    X_test_wb,\n",
        "    tf.keras.utils.to_categorical(y_test_manipulated, num_classes=10)\n",
        ")\n",
        "\n",
        "print(f\"Renk sabitliği uygulanmış test setindeki kayıp: {test_loss_wb}\")\n",
        "print(f\"Renk sabitliği uygulanmış test setindeki doğruluk: {test_accuracy_wb}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a5AgDBBzFo",
        "outputId": "2c1dc8f0-37d5-43ec-d212-d9ef1205e5f7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6468 - loss: 5.6075\n",
            "Renk sabitliği uygulanmış test setindeki kayıp: 5.261206150054932\n",
            "Renk sabitliği uygulanmış test setindeki doğruluk: 0.6529914736747742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Comparing and Reporting the Success of Different Test Sets:"
      ],
      "metadata": {
        "id": "awM3hOqXBz4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Initial Test Data Results**\n",
        "\n",
        "Accuracy: 67.56%\n",
        "Loss: 5.5746\n",
        "\n",
        "The results on the initial test data show an accuracy of 67.56% and a loss of 5.5746. This indicates that while the model is able to make reasonably accurate predictions, it is not perfect. The model's general performance is moderate, and there is potential for improvement through fine-tuning and addressing the issues that arise from the data distribution and model architecture.\n",
        "\n",
        "**2. Manipulated Test Data Results**\n",
        "\n",
        "Accuracy: 18.62%\n",
        "Loss: 54.2937\n",
        "\n",
        "The results on the manipulated test data show a significant drop in performance, with accuracy at just 18.62% and a loss of 54.2937. This low accuracy suggests that the manipulations, which alter the natural structure of the data, made it difficult for the model to make accurate predictions. The manipulations likely changed the image characteristics in ways that the model wasn't trained to handle effectively.\n",
        "\n",
        "**3. White-Balanced Test Data Results**\n",
        "\n",
        "Accuracy: 65.30%\n",
        "Loss: 5.2612\n",
        "\n",
        "The results on the white-balanced test data show an accuracy of 65.30% and a loss of 5.2612. The performance here is closer to the initial test data, indicating that the white-balancing algorithm helped reduce the negative impact of the manipulations. While still not perfect, the results suggest that applying a color correction technique like white balancing can improve the model's ability to handle manipulated images and restore a more balanced performance.\n"
      ],
      "metadata": {
        "id": "R6BK7iKEB-Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the Results and Possible Issues**\n",
        "\n",
        "1. Impact of Manipulations on Model Performance\n",
        "\n",
        "Manipulations typically change the inherent properties of the data, making it harder for the model to predict accurately. Some possible reasons for the poor performance on the manipulated test data include:\n",
        "\n",
        "Data Distribution Mismatch: The model trained on the original dataset might not generalize well to manipulated images due to significant changes in the lighting, colors, or other features. The manipulation may have altered the distribution of the data in a way the model was not trained to handle.\n",
        "Model Underfitting or Overfitting: The model might not have been trained with sufficient variety in the data to learn the effects of manipulations. This can result in the model underperforming when it encounters manipulated data.\n",
        "\n",
        "2. Limitations of the White-Balancing Algorithm\n",
        "\n",
        "While the white-balancing algorithm improved performance on manipulated data, it may not be sufficient in all cases. The improvement was notable but still did not bring the performance back to the level of the initial test data. Some factors contributing to the limited success of white balancing include:\n",
        "\n",
        "Inadequate Algorithm: The white-balancing method may not have been strong enough to counteract all types of manipulation, especially if the changes in color and lighting were extreme or non-linear.\n",
        "Complex Lighting Conditions: The white-balancing technique might struggle to account for complex or dynamic lighting conditions, reducing its effectiveness in real-world scenarios."
      ],
      "metadata": {
        "id": "Xpt3CMmh5Xti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution Suggestions**\n",
        "\n",
        "Training with More Manipulated Data:\n",
        "\n",
        "To improve the model’s resilience to manipulations, it would be beneficial to train it with more manipulated data. By including images with a variety of manipulations (e.g., color changes, lighting shifts), the model can better generalize and become more robust to such variations.\n",
        "Additionally, data augmentation techniques can be applied during training to artificially increase the diversity of the dataset, such as through rotation, scaling, or introducing synthetic lighting changes.\n",
        "\n",
        "Enhancing the White-Balancing Algorithm:\n",
        "\n",
        "The current white-balancing algorithm might need to be more sophisticated. Exploring more advanced white-balancing techniques, such as those based on machine learning or deep learning models, could provide better results under diverse conditions.\n",
        "For example, algorithms like color constancy methods or learning-based color correction techniques could be explored to improve color consistency and lighting corrections in the manipulated images.\n",
        "Improving the Model Architecture and Hyperparameters:\n",
        "\n",
        "The model architecture itself might need to be refined. Experimenting with deeper architectures, like ResNet or VGG, or leveraging transfer learning from pre-trained models might lead to better results.\n",
        "Also, tuning the learning rate and batch size, as well as adjusting the number of epochs and early stopping parameters, may lead to better model performance.\n",
        "Utilizing More Diverse Manipulations:\n",
        "\n",
        "In addition to color manipulations, other forms of image augmentations (such as brightness, contrast, or saturation variations) can be included in the training data to better simulate real-world variations.\n",
        "This would allow the model to handle a broader range of manipulations during inference.\n",
        "\n",
        "Monitoring Model Performance Regularly:\n",
        "\n",
        "It’s important to monitor the model’s performance on a validation set to identify signs of overfitting or underfitting. Regular evaluations during training can help catch issues early and adjust the model’s learning process accordingly.\n"
      ],
      "metadata": {
        "id": "yEhVzZNT5oI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The manipulated and white-balanced test data results highlight challenges in handling manipulated images, but also suggest that techniques like white balancing can mitigate some of these issues. However, the overall performance still falls short of expectations. Implementing the suggested improvements, such as training with more diverse manipulated data, improving the white-balancing algorithm, and refining the model architecture, will likely lead to better handling of manipulated and complex real-world data."
      ],
      "metadata": {
        "id": "qwDWXNpi6DJa"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}